# This .pyi is auto generated by the script in the root folder.
# only for cache,use .py for changes
import os

import cv2
import numpy as np
import torch

from toddleocr.config import _, ConfigModel, PROJECT_DIR
from toddleocr.datasets.simple import SimpleDataSet
from toddleocr.loss.basic import LossFromOutput
from toddleocr.metrics.vqa import VQAReTokenMetric
from toddleocr.modules.backbones.vqa_layoutlm import LayoutXLMForRe
from toddleocr.postprocess.vqa import VQAReTokenLayoutLMPostProcess
from toddleocr.transforms import VQATokenLabelEncode
from toddleocr.transforms.operators import (
    DecodeImage,
    KeepKeys,
    NormalizeImage,
    Resize,
    ToCHWImage,
)
from toddleocr.transforms.vqa.token.vqa_re_convert import TensorizeEntitiesRelations
from toddleocr.transforms.vqa.token.vqa_token_chunk import VQAReTokenChunk
from toddleocr.transforms.vqa.token.vqa_token_pad import VQATokenPad
from toddleocr.transforms.vqa.token.vqa_token_relation import VQAReTokenRelation
from torch.optim import AdamW
from torch.optim.lr_scheduler import ConstantLR

from utils.visual import draw_re_results

CLASS_PATH = os.path.join(PROJECT_DIR, "../train_data/XFUND/class_list_xfun.txt")
from toddleocr import ToddleOCR


class Model(ConfigModel):
    use_gpu = True
    epoch_num = 130
    log_window_size = 10
    log_batch_step = 10
    save_model_dir = None
    save_epoch_step = 2000
    eval_batch_step = [0, 19]
    metric_during_train = False
    save_infer_dir = None
    use_visualdl = False
    seed = 2022
    pretrained_model = None
    model_type = "kie"
    algorithm = "LayoutXLM"
    Transform = None
    Backbone = _(LayoutXLMForRe,
                 pretrained="D:/dev/.model/huggingface/layoutxlm-base",
                 checkpoints=None)
    loss = LossFromOutput(key="loss", reduction="mean")
    metric = VQAReTokenMetric(main_indicator="hmean")
    postprocessor = VQAReTokenLayoutLMPostProcess()
    Optimizer = _(AdamW, beta1=0.9, beta2=0.999, clip_norm=10, lr=5e-05)
    LRScheduler = _(ConstantLR, warmup_epoch=10)

    class Data:
        dataset = SimpleDataSet
        root: "train_data/XFUND/zh_val/image" = "train_data/XFUND/zh_train/image"
        label_file_list: "train_data/XFUND/zh_val/val.json" = (
            "train_data/XFUND/zh_train/train.json"
        )

    class Loader:
        shuffle: False = True
        drop_last = False
        batch_size: 8 = 2
        num_workers = 8

    kie_det_model_dir = os.path.join(PROJECT_DIR, "../weights/zh_ocr_det_v3")
    kie_rec_model_dir = os.path.join(PROJECT_DIR, "../weights/zh_ocr_rec_v3")

    # ocr_engine = ToddleOCR(
    #     det_model_dir=kie_det_model_dir,
    #     rec_model_dir=kie_rec_model_dir,
    #     use_gpu=use_gpu,
    #     use_angle_cls=False,
    # )

    Transforms = _[
                 DecodeImage(img_mode="RGB", channel_first=False),
                 VQATokenLabelEncode(
                     contains_re=True,
                     algorithm="LayoutXLM",
                     class_path=CLASS_PATH,
                 ): ...:VQATokenLabelEncode(
                     contains_re=False,
                     algorithm="LayoutXLM",
                     class_path=CLASS_PATH,
                     # ocr_engine=ocr_engine,
                     infer_mode=True,
                 ),
                 VQATokenPad(max_seq_len=512, return_attention_mask=True),
                 VQAReTokenRelation(),
                 VQAReTokenChunk(max_seq_len=512),
                 TensorizeEntitiesRelations(),
                 Resize(size=[224, 224]),
                 NormalizeImage(
                     scale=1,
                     mean=[123.675, 116.28, 103.53],
                     std=[58.395, 57.12, 57.375],
                     order="hwc",
                 ),
                 ToCHWImage(),
                 KeepKeys(
                     "input_ids",
                     "bbox",
                     "attention_mask",
                     "token_type_ids",
                     "image",
                     "entities",
                     "relations",
                 ): ...,
                 ]

    @torch.no_grad()
    def re_one_image(self, img_or_path,ser_engine, output_dir=None):
        ser_path = os.path.join(output_dir, 'ser_' + os.path.basename(img_or_path))
        re_path = os.path.join(output_dir, 're_' + os.path.basename(img_or_path))

        ser_results, ser_inputs = ser_engine.ser_one_image(img_or_path, ser_path)

        re_input, entity_idx_dict_batch = make_input(ser_inputs, ser_results)
        if self.model.backbone.use_visual_backbone is False:
            re_input.pop(4)
        self.model.eval()
        preds = self.model(re_input) #是个列表
        print(preds)
        post_result = self.postprocessor(
            preds,
            ser_results=ser_results,
            entity_idx_dict_batch=entity_idx_dict_batch)[0]

        print(post_result)
        img_res = draw_re_results(img_or_path, post_result)
        cv2.imwrite(re_path, img_res)
        return post_result


def make_input(ser_inputs, ser_results):
    entities_labels = {'HEADER': 0, 'QUESTION': 1, 'ANSWER': 2}
    batch_size, max_seq_len = ser_inputs[0].shape[:2]
    entities = ser_inputs[8][0]
    ser_results = ser_results[0]
    assert len(entities) == len(ser_results)

    # entities
    start = []
    end = []
    label = []
    entity_idx_dict = {}
    for i, (res, entity) in enumerate(zip(ser_results, entities)):
        if res['pred'] == 'O':
            continue
        entity_idx_dict[len(start)] = i
        start.append(entity['start'])
        end.append(entity['end'])
        label.append(entities_labels[res['pred']])

    entities = np.full([max_seq_len + 1, 3], fill_value=-1, dtype=np.int64)
    entities[0, 0] = len(start)
    entities[1:len(start) + 1, 0] = start
    entities[0, 1] = len(end)
    entities[1:len(end) + 1, 1] = end
    entities[0, 2] = len(label)
    entities[1:len(label) + 1, 2] = label

    # relations
    head = []
    tail = []
    for i in range(len(label)):
        for j in range(len(label)):
            if label[i] == 1 and label[j] == 2:
                head.append(i)
                tail.append(j)

    relations = np.full([len(head) + 1, 2], fill_value=-1, dtype=np.int64)
    relations[0, 0] = len(head)
    relations[1:len(head) + 1, 0] = head
    relations[0, 1] = len(tail)
    relations[1:len(tail) + 1, 1] = tail

    entities = np.expand_dims(entities, axis=0)
    entities = np.repeat(entities, batch_size, axis=0)
    relations = np.expand_dims(relations, axis=0)
    relations = np.repeat(relations, batch_size, axis=0)

    # remove ocr_info segment_offset_id and label in ser input
    if isinstance(ser_inputs[0], torch.Tensor):
        entities = torch.tensor(entities)
        relations = torch.tensor(relations)
    ser_inputs = ser_inputs[:5] + [entities, relations]

    entity_idx_dict_batch = []
    for b in range(batch_size):
        entity_idx_dict_batch.append(entity_idx_dict)
    return ser_inputs, entity_idx_dict_batch


def _t():
    global endswith
    import torch
    import paddle
    def endswith(p, ls):
        for s in ls:
            if p.endswith(s):
                return True
        return False

    def transmodel(pdmodel, linear_suffix=()):
        def p2t(tensor) -> torch.Tensor:
            return torch.from_numpy(tensor.numpy())

        # global transpose
        pd = paddle.load(pdmodel)
        maps = {'._mean': '.running_mean',
                '._variance': '.running_var',
                'layoutxlm': 'backbone.model.layoutxlm',  # todo,simplify the prefix of backbone
                # 'classifier': 'backbone.model.classifier',
                'extractor': 'backbone.model.extractor',
                }
        transpose = 'weight'
        new = {}
        for k, v in pd.items():
            tk = k
            for key in maps:
                if key in k:
                    tk = tk.replace(key, maps[key])
            new[tk] = p2t(v)

        for tk in new.keys():
            if tk.endswith(transpose):
                if endswith(tk, linear_suffix):
                    new[tk] = new[tk].T

        torch.save(new, fr'{pdmodel.split(".")[0]}.pt')

    transmodel(r"D:\dev\github\ToddleOCR\model\re_LayoutXLM_xfun_zh\model_state.pdparams",
               ("classifier.weight", "query.weight", "key.weight", "value.weight", "dense.weight",
                "visual_proj.weight", ".linear.weight", "ffnn_head.0.weight", "ffnn_head.3.weight",
                "ffnn_tail.0.weight",
                "ffnn_tail.3.weight"
                ))


# if __name__ == '__main__':
    # _t()

    # for k in m.model.state_dict():
    #     print(k)
