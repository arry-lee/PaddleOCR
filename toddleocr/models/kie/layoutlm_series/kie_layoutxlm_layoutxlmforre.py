# This .pyi is auto generated by the script in the root folder.
# only for cache,use .py for changes
import os

from toddleocr.config import _, ConfigModel, PROJECT_DIR
from toddleocr.datasets.simple import SimpleDataSet
from toddleocr.loss.basic import LossFromOutput
from toddleocr.metrics.vqa import VQAReTokenMetric
from toddleocr.modules.backbones.vqa_layoutlm import LayoutXLMForRe
from toddleocr.postprocess.vqa import VQAReTokenLayoutLMPostProcess
from toddleocr.transforms import VQATokenLabelEncode
from toddleocr.transforms.operators import (
    DecodeImage,
    KeepKeys,
    NormalizeImage,
    Resize,
    ToCHWImage,
)
from toddleocr.transforms.vqa.token.vqa_re_convert import TensorizeEntitiesRelations
from toddleocr.transforms.vqa.token.vqa_token_chunk import VQAReTokenChunk
from toddleocr.transforms.vqa.token.vqa_token_pad import VQATokenPad
from toddleocr.transforms.vqa.token.vqa_token_relation import VQAReTokenRelation
from torch.optim import AdamW
from torch.optim.lr_scheduler import ConstantLR

CLASS_PATH = os.path.join(PROJECT_DIR, "../train_data/XFUND/class_list_xfun.txt")
from toddleocr import ToddleOCR

class Model(ConfigModel):
    use_gpu = True
    epoch_num = 130
    log_window_size = 10
    log_batch_step = 10
    save_model_dir = None
    save_epoch_step = 2000
    eval_batch_step = [0, 19]
    metric_during_train = False
    save_infer_dir = None
    use_visualdl = False
    seed = 2022
    pretrained_model = None
    model_type = "kie"
    algorithm = "LayoutXLM"
    Transform = None
    Backbone = _(LayoutXLMForRe,
                 pretrained="D:/dev/.model/huggingface/layoutxlm-base",
                 checkpoints=None)
    loss = LossFromOutput(key="loss", reduction="mean")
    metric = VQAReTokenMetric(main_indicator="hmean")
    postprocessor = VQAReTokenLayoutLMPostProcess()
    Optimizer = _(AdamW, beta1=0.9, beta2=0.999, clip_norm=10, lr=5e-05)
    LRScheduler = _(ConstantLR, warmup_epoch=10)

    class Data:
        dataset = SimpleDataSet
        root: "train_data/XFUND/zh_val/image" = "train_data/XFUND/zh_train/image"
        label_file_list: "train_data/XFUND/zh_val/val.json" = (
            "train_data/XFUND/zh_train/train.json"
        )

    class Loader:
        shuffle: False = True
        drop_last = False
        batch_size: 8 = 2
        num_workers = 8

    kie_det_model_dir = os.path.join(PROJECT_DIR, "../weights/zh_ocr_det_v3")
    kie_rec_model_dir = os.path.join(PROJECT_DIR, "../weights/zh_ocr_rec_v3")

    ocr_engine = ToddleOCR(
        det_model_dir=kie_det_model_dir,
        rec_model_dir=kie_rec_model_dir,
        use_gpu=use_gpu,
        use_angle_cls=False,
    )

    Transforms = _[
        DecodeImage(img_mode="RGB", channel_first=False),
        VQATokenLabelEncode(
            contains_re=True,
            algorithm="LayoutXLM",
            class_path=CLASS_PATH,
        ) : ...:VQATokenLabelEncode(
                     contains_re=False,
                     algorithm="LayoutXLM",
                     class_path=CLASS_PATH,
                     ocr_engine=ocr_engine,
                     infer_mode=True,
                 ),
        VQATokenPad(max_seq_len=512, return_attention_mask=True),
        VQAReTokenRelation(),
        VQAReTokenChunk(max_seq_len=512),
        TensorizeEntitiesRelations(),
        Resize(size=[224, 224]),
        NormalizeImage(
            scale=1,
            mean=[123.675, 116.28, 103.53],
            std=[58.395, 57.12, 57.375],
            order="hwc",
        ),
        ToCHWImage(),
        KeepKeys(
            "input_ids",
            "bbox",
            "attention_mask",
            "token_type_ids",
            "image",
            "entities",
            "relations",
        ) : ...,
    ]

def _t():
    global endswith
    import torch
    import paddle
    def endswith(p, ls):
        for s in ls:
            if p.endswith(s):
                return True
        return False

    def transmodel(pdmodel, linear_suffix=()):
        def p2t(tensor) -> torch.Tensor:
            return torch.from_numpy(tensor.numpy())

        # global transpose
        pd = paddle.load(pdmodel)
        maps = {'._mean': '.running_mean',
                '._variance': '.running_var',
                'layoutxlm': 'backbone.model.layoutxlm', # todo,simplify the prefix of backbone
                # 'classifier': 'backbone.model.classifier',
                'extractor':'backbone.model.extractor',
                }
        transpose = 'weight'
        new = {}
        for k, v in pd.items():
            tk = k
            for key in maps:
                if key in k:
                    tk = tk.replace(key, maps[key])
            new[tk] = p2t(v)

        for tk in new.keys():
            if tk.endswith(transpose):
                if endswith(tk, linear_suffix):
                    new[tk] = new[tk].T

        torch.save(new, fr'{pdmodel.split(".")[0]}.pt')

    transmodel(r"D:\dev\github\ToddleOCR\model\re_LayoutXLM_xfun_zh\model_state.pdparams",
               ("classifier.weight", "query.weight", "key.weight", "value.weight", "dense.weight",
                "visual_proj.weight",".linear.weight","ffnn_head.0.weight","ffnn_head.3.weight","ffnn_tail.0.weight",
                "ffnn_tail.3.weight"
                ))

if __name__ == '__main__':
    _t()
    m = Model(pretrained=r"D:\dev\github\ToddleOCR\model\re_LayoutXLM_xfun_zh\model_state.pt")
    for k in m.model.state_dict():
        print(k)
