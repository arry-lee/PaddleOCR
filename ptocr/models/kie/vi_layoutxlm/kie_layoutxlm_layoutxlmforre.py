# This .py is auto generated by the script in the root folder.
from ptocr.config import ConfigModel,_
from ptocr.modules.backbones.vqa_layoutlm import LayoutXLMForRe
from ptocr.loss.basic import LossFromOutput
from ptocr.metrics.vqa import VQAReTokenMetric
from ptocr.postprocess.vqa import VQAReTokenLayoutLMPostProcess
from torch.optim import AdamW
from torch.optim.lr_scheduler import ConstantLR
from ptocr.datasets.simple import SimpleDataSet
from ptocr.transforms.operators import ToCHWImage, KeepKeys, NormalizeImage, DecodeImage, Resize
from ptocr.transforms.label_ops import VQATokenLabelEncode
from ptocr.transforms.vqa.token.vqa_token_pad import VQATokenPad
from ptocr.transforms.vqa.token.vqa_token_relation import VQAReTokenRelation
from ptocr.transforms.vqa.token.vqa_token_chunk import VQAReTokenChunk
from ptocr.transforms.vqa.token.vqa_re_convert import TensorizeEntitiesRelations
class Model(ConfigModel):
    use_gpu = True
    epoch_num = 130
    log_window_size = 10
    log_batch_step = 10
    save_model_dir = "./output/re_vi_layoutxlm_xfund_zh"
    save_epoch_step = 2000
    eval_batch_step = [0, 19]
    metric_during_train = False
    save_infer_dir = None
    use_visualdl = False
    seed = 2022
    infer_img = "ppstructure/docs/kie/input/zh_val_21.jpg"
    save_res_path = "./output/re/xfund_zh/with_gt"
    kie_rec_model_dir = None
    kie_det_model_dir = None
    model_type = 'kie'
    algorithm = 'LayoutXLM'
    Transform = None
    Backbone = _(LayoutXLMForRe, pretrained=True, mode="vi", checkpoints=None)
    loss = LossFromOutput(key="loss", reduction="mean")
    metric = VQAReTokenMetric(main_indicator="hmean")
    postprocessor = VQAReTokenLayoutLMPostProcess()
    Optimizer = _(AdamW,beta1=0.9, beta2=0.999, clip_norm=10, lr=5e-05)
    LRScheduler = _(ConstantLR,warmup_epoch=10)
    class Train:
        Dataset = _(SimpleDataSet, root="train_data/XFUND/zh_train/image", label_files=['train_data/XFUND/zh_train/train.json'], ratio_list=[1.0])
        transforms = _[DecodeImage(img_mode="RGB", channel_first=False), VQATokenLabelEncode(contains_re=True, algorithm="LayoutXLM", class_path="train_data/XFUND/class_list_xfun.txt", use_textline_bbox_info=True, order_method="tb-yx"), VQATokenPad(max_seq_len=512, return_attention_mask=True), VQAReTokenRelation(), VQAReTokenChunk(max_seq_len=512), TensorizeEntitiesRelations(), Resize(size=[224, 224]), NormalizeImage(scale=1, mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], order="hwc"), ToCHWImage(), KeepKeys(keep_keys=['input_ids', 'bbox', 'attention_mask', 'token_type_ids', 'entities', 'relations'])]
        DATALOADER = _(shuffle=True, drop_last=False, batch_size=2, num_workers=4)
    class Eval:
        Dataset = _(SimpleDataSet, root="train_data/XFUND/zh_val/image", label_files=['train_data/XFUND/zh_val/val.json'])
        transforms = _[DecodeImage(img_mode="RGB", channel_first=False), VQATokenLabelEncode(contains_re=True, algorithm="LayoutXLM", class_path="train_data/XFUND/class_list_xfun.txt", use_textline_bbox_info=True, order_method="tb-yx"), VQATokenPad(max_seq_len=512, return_attention_mask=True), VQAReTokenRelation(), VQAReTokenChunk(max_seq_len=512), TensorizeEntitiesRelations(), Resize(size=[224, 224]), NormalizeImage(scale=1, mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], order="hwc"), ToCHWImage(), KeepKeys(keep_keys=['input_ids', 'bbox', 'attention_mask', 'token_type_ids', 'entities', 'relations'])]
        DATALOADER = _(shuffle=False, drop_last=False, batch_size=8, num_workers=8)
